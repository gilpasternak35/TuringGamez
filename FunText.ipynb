{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "import pandas as pd\n",
    "import requests, sys, webbrowser,xml\n",
    "import numpy as np\n",
    "import bs4\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline usage to perform test generation, using the hugging face package transformers. Citation is below:\n",
    "@article{Wolf2019HuggingFacesTS,\n",
    "  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},\n",
    "  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},\n",
    "  journal={ArXiv},\n",
    "  year={2019},\n",
    "  volume={abs/1910.03771}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startPipeline():\n",
    "    generator = pipeline('fill-mask', model='bert-base-uncased')\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = startPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Given a number letters and some sequence, returns the sequence with generated attached words\n",
    "def generateText(generator,intro_sequence:str, num_words = 5)->str:\n",
    "    # Imported random text generation\n",
    "    text = generator(intro_sequence, max_length=len(intro_sequence) + num_words, num_return_sequences=1)[0].get(\"generated_text\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(url = \"https://www.keepinspiring.me/famous-quotes/\" ):\n",
    "    # Requesting data from url, finding specialized tags for this particular website\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(res.text, \"html.parser\")\n",
    "    text  = soup.find_all(\"div\", class_ = 'author-quotes')  \n",
    "    return text\n",
    "text = getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game List: \n",
    "1) **Mad Libs** \n",
    "\n",
    "2) **Find the real quote**\n",
    "\n",
    "3) **How well do you know your favorite song?**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Processing Below\n",
    "Scraping the data from our quote website and cleaning it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtainin author from quote\n",
    "def authors(quote):\n",
    "    return quote.split(\"”\")[1]\n",
    "\n",
    "#Removing author and adding lost smartquote\n",
    "def removeAuthors(quote):\n",
    "    return (quote.split(\"”\")[0] + (\"”\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tag processing functions to clean up nasty html formatting, replaces div tags\n",
    "def processing_div(tag):\n",
    "    return tag.replace('<div class=\"author-quotes\">', \"\").replace(\"</div>\", \"\")\n",
    "\n",
    "#span processing, replaces span tag\n",
    "def processing_span(tag):\n",
    "    return tag.replace(\"<span class=\\\"quote-author-name\\\">\", \"\").replace(\"</span>\", \"\")\n",
    "\n",
    "# Checks for tags that have yet to be removed, not given standard format. Reasoning - we don't know when ads will pop up\n",
    "def cleaner(table):\n",
    "    arr = np.array([])\n",
    "    for i in table.get(\"quote\"):\n",
    "        arr = np.append(arr,(\"<\" in i))\n",
    "    clean = table[(arr != 1)]\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“You know you’re in love when you can’t fall a...</td>\n",
       "      <td>– Dr. Suess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“I’m selfish, impatient and a little insecure....</td>\n",
       "      <td>– Marilyn Monroe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“Get busy living or get busy dying.”</td>\n",
       "      <td>– Stephen King</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“The first step toward success is taken when y...</td>\n",
       "      <td>– Mark Caine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>“Twenty years from now you will be more disapp...</td>\n",
       "      <td>– Mark Twain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>“The dream crossed twilight between birth and ...</td>\n",
       "      <td>– T. S. Eliot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>“Don’t think. Thinking is the enemy of creativ...</td>\n",
       "      <td>– Ray Bradbury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>“The power of imagination makes us infinite.”</td>\n",
       "      <td>– John Muir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>“Originality is nothing but judicious imitation.”</td>\n",
       "      <td>– Voltaire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>“Life is made of ever so many partings welded ...</td>\n",
       "      <td>– Charles Dickens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                quote              author\n",
       "0   “You know you’re in love when you can’t fall a...         – Dr. Suess\n",
       "1   “I’m selfish, impatient and a little insecure....    – Marilyn Monroe\n",
       "2                “Get busy living or get busy dying.”      – Stephen King\n",
       "3   “The first step toward success is taken when y...        – Mark Caine\n",
       "5   “Twenty years from now you will be more disapp...        – Mark Twain\n",
       "..                                                ...                 ...\n",
       "83  “The dream crossed twilight between birth and ...       – T. S. Eliot\n",
       "84  “Don’t think. Thinking is the enemy of creativ...      – Ray Bradbury\n",
       "86      “The power of imagination makes us infinite.”         – John Muir\n",
       "88  “Originality is nothing but judicious imitation.”          – Voltaire\n",
       "89  “Life is made of ever so many partings welded ...   – Charles Dickens\n",
       "\n",
       "[72 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.DataFrame().assign(quote = text)\n",
    "\n",
    "#Formatting, processing, and splitting quotes and authors\n",
    "def tableProcess(table):\n",
    "    table = table.assign(quote = table.get(\"quote\").apply(str))\n",
    "    table = table.assign(quote = table.get(\"quote\").apply(processing_div).apply(processing_span))\n",
    "    table = cleaner(table)\n",
    "    table = table.assign(author  = table.get(\"quote\").apply(authors), quote = table.get(\"quote\").apply(removeAuthors))\n",
    "    return table\n",
    "table = tableProcess(table)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Word Replacement to be used in each individual Turing game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def replace_words_at_random(generator, word_arr: [str], num_words: int):\n",
    "    \n",
    "    #  Generate num_words random indices\n",
    "    indices = randomGeneration(num_words, 0, len(word_arr))\n",
    "    print(indices)\n",
    "    # Place word masks at each of the randomly chosen indices\n",
    "    # Fill in each mask with the language model\n",
    "    for i in indices:\n",
    "        #Ensuring array is not overaccessed\n",
    "        if(i < len(word_arr)):\n",
    "            word_arr[i] = '[MASK]'\n",
    "        \n",
    "        #Adding together everything leading up to string so as to add some context into the model\n",
    "        join  = \" \".join(word_arr)\n",
    "        print(join)\n",
    "\n",
    "        # Generate the next word\n",
    "        text = (generator(join)[0].get('sequence').replace(\"[CLS]\", '').replace('[SEP]', '')).strip()\n",
    "        word_arr = text.split(\" \")\n",
    "\n",
    "    return text, indices \n",
    "\n",
    "# Generates a desired number of unique random digits in a certain range\n",
    "def randomGeneration(num_words, lowNum, highNum):\n",
    "    random_digits = np.unique(np.random.randint(low = lowNum, high  = highNum, size = num_words))\n",
    "    while(len(random_digits) < num_words):\n",
    "        random_digits = np.append(random_digits, np.random.randint(low = lowNum, high = highNum, size = num_words-len(random_digits)))\n",
    "        random_digits = np.unique(random_digits)\n",
    "    return random_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia Search Below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start at user given wikipedia page, randomly click a certain number of links from there and scrape 5 sentences from the final page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWikiText(url):\n",
    "    # Requesting data from url, finding specialized tags for this particular website\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(res.text, \"lxml\")\n",
    "    text = \"\"\n",
    "    for paragraph in soup.find_all('p'):\n",
    "        text+= paragraph.text\n",
    "    text = re.sub(r'\\[[0-9]*\\]','', text)\n",
    "    text= re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\d', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterates through wikipedia pages\n",
    "def wiki_search():\n",
    "    topic, pages = inp()\n",
    "    valid_url = construct_wiki_url(topic)\n",
    "    textSoup = getWikiText(valid_url)\n",
    "    #print(textSoup)\n",
    "    print(textSoup.split(' ')[5])\n",
    "\n",
    "    \n",
    "    \n",
    "# Loop through connected topics on wikipedia to find a \"landing page\", then return the url of that landing page\n",
    "\"\"\"def looping_wiki_search(base_topic, neighbor_pages):\n",
    "    current_topic, pages = inp()\n",
    "    for i in range(pages): \n",
    "        url = construct_wiki_url(current_topic)\n",
    "        current_topic ... get some new topic from url\"\"\"\n",
    "        \n",
    "        \n",
    "# A function which accepts a wikipedia URL and scrapes a \"story\" from the text on that page\n",
    "def scrape_wikipedia_story(wiki_url: str) -> str:\n",
    "    pass\n",
    "\n",
    "# Temporary dummy input\n",
    "def inp():\n",
    "    print(\"Please enter a topic\")\n",
    "    val = str(input())\n",
    "    print('Please enter the number of neighbor pages: ')\n",
    "    pages = input()\n",
    "    return val, pages\n",
    "\n",
    "# Takes a given user topic and constructs a valid wikipedia url\n",
    "def construct_wiki_url(url_topic : str):\n",
    "    return ('https://en.wikipedia.org/wiki/' + url_topic.strip().replace(\" \",\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
