{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "import pandas as pd\n",
    "import requests, sys, webbrowser,xml\n",
    "import numpy as np\n",
    "import bs4\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline usage to perform test generation, using the hugging face package transformers. Citation is below:\n",
    "@article{Wolf2019HuggingFacesTS,\n",
    "  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},\n",
    "  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},\n",
    "  journal={ArXiv},\n",
    "  year={2019},\n",
    "  volume={abs/1910.03771}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting up pipeline\n",
    "def start_pipeline():\n",
    "    generator = pipeline('fill-mask', model='bert-base-uncased')\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "nlp = start_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Given a number letters and some sequence, returns the sequence with generated attached words\n",
    "def generate_text(generator,intro_sequence:str, num_words = 5)->str:\n",
    "    # Imported random text generation\n",
    "    text = generator(intro_sequence, max_length=len(intro_sequence) + num_words, num_return_sequences=1)[0].get(\"generated_text\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(url = \"https://www.keepinspiring.me/famous-quotes/\" ):\n",
    "    # Requesting data from url, finding specialized tags for this particular website\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(res.text, \"html.parser\")\n",
    "    text  = soup.find_all(\"div\", class_ = 'author-quotes')  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game List: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Format: select the game -> select the text type*\n",
    "\n",
    "1) **Mad Libs** \n",
    "<ul>\n",
    "    <li>Favorite song Edition</li>\n",
    "    <li>Wikipedia edition</li>\n",
    "    <li>All time famous quotes edition</li>\n",
    "    <li>Generate a normal mad-lib using sentiment analysis to get parts of speech, tell user in random 'noun,verb, etc.'</li>\n",
    "</ul>\n",
    "\n",
    "2) **Find the real text**\n",
    "<ul>\n",
    "    <li>Favorite song Edition</li>\n",
    "    <li>Wikipedia edition</li>\n",
    "    <li>All time famous quotes edition</li>\n",
    "</ul>\n",
    "\n",
    "3) **How well do you know your favorite song?**\n",
    "\n",
    "4) **Can you guess what Wikipedia page this used to be?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Processing Below\n",
    "Scraping the data from our quote website and cleaning it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtainin author from quote\n",
    "def authors(quote):\n",
    "    return quote.split(\"”\")[1]\n",
    "\n",
    "#Removing author and adding lost smartquote\n",
    "def remove_authors(quote):\n",
    "    return (quote.split(\"”\")[0] + (\"”\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tag processing functions to clean up nasty html formatting, replaces div tags\n",
    "def processing_div(tag):\n",
    "    return tag.replace('<div class=\"author-quotes\">', \"\").replace(\"</div>\", \"\")\n",
    "\n",
    "#span processing, replaces span tag\n",
    "def processing_span(tag):\n",
    "    return tag.replace(\"<span class=\\\"quote-author-name\\\">\", \"\").replace(\"</span>\", \"\")\n",
    "\n",
    "# Checks for tags that have yet to be removed, not given standard format. Reasoning - we don't know when ads will pop up\n",
    "def cleaner(table):\n",
    "    arr = np.array([])\n",
    "    for i in table.get(\"quote\"):\n",
    "        arr = np.append(arr,(\"<\" in i))\n",
    "    clean = table[(arr != 1)]\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“You know you’re in love when you can’t fall a...</td>\n",
       "      <td>– Dr. Suess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“I’m selfish, impatient and a little insecure....</td>\n",
       "      <td>– Marilyn Monroe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“Get busy living or get busy dying.”</td>\n",
       "      <td>– Stephen King</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“The first step toward success is taken when y...</td>\n",
       "      <td>– Mark Caine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>“Twenty years from now you will be more disapp...</td>\n",
       "      <td>– Mark Twain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>“The dream crossed twilight between birth and ...</td>\n",
       "      <td>– T. S. Eliot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>“Don’t think. Thinking is the enemy of creativ...</td>\n",
       "      <td>– Ray Bradbury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>“The power of imagination makes us infinite.”</td>\n",
       "      <td>– John Muir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>“Originality is nothing but judicious imitation.”</td>\n",
       "      <td>– Voltaire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>“Life is made of ever so many partings welded ...</td>\n",
       "      <td>– Charles Dickens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                quote              author\n",
       "0   “You know you’re in love when you can’t fall a...         – Dr. Suess\n",
       "1   “I’m selfish, impatient and a little insecure....    – Marilyn Monroe\n",
       "2                “Get busy living or get busy dying.”      – Stephen King\n",
       "3   “The first step toward success is taken when y...        – Mark Caine\n",
       "5   “Twenty years from now you will be more disapp...        – Mark Twain\n",
       "..                                                ...                 ...\n",
       "83  “The dream crossed twilight between birth and ...       – T. S. Eliot\n",
       "84  “Don’t think. Thinking is the enemy of creativ...      – Ray Bradbury\n",
       "86      “The power of imagination makes us infinite.”         – John Muir\n",
       "88  “Originality is nothing but judicious imitation.”          – Voltaire\n",
       "89  “Life is made of ever so many partings welded ...   – Charles Dickens\n",
       "\n",
       "[72 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.DataFrame().assign(quote = text)\n",
    "\n",
    "#Formatting, processing, and splitting quotes and authors\n",
    "def table_process(table):\n",
    "    table = table.assign(quote = table.get(\"quote\").apply(str))\n",
    "    table = table.assign(quote = table.get(\"quote\").apply(processing_div).apply(processing_span))\n",
    "    table = cleaner(table)\n",
    "    table = table.assign(author  = table.get(\"quote\").apply(authors), quote = table.get(\"quote\").apply(remove_authors))\n",
    "    return table\n",
    "table = table_process(table)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Word Replacement to be used in each individual Turing game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words_at_random(generator, word_arr: [str], num_words: int, difficulty = 1) -> ([str],[int]):\n",
    "    \n",
    "    #  Generate num_words random indices\n",
    "    indices = random_generation(num_words, 0, len(word_arr))\n",
    "    print(indices)\n",
    "    \n",
    "    # Place word masks at each of the randomly chosen indices\n",
    "    # Fill in each mask with the language model\n",
    "    for i in indices:\n",
    "        #Ensuring array is not overaccessed\n",
    "        if(i < len(word_arr)):\n",
    "            word_arr[i] = '[MASK]'\n",
    "        \n",
    "        #Adding together everything leading up to string so as to add some context into the model\n",
    "        join  = \" \".join(word_arr)\n",
    "\n",
    "        # Generate the next word\n",
    "        text = (generator(join)[3-difficulty].get('sequence').replace(\"[CLS]\", '').replace('[SEP]', '')).strip()\n",
    "        word_arr = text.split(\" \")\n",
    "\n",
    "    return text, indices \n",
    "\n",
    "# Generates a desired number of unique random digits in a certain range\n",
    "def random_generation(num_words, lowNum, highNum):\n",
    "    random_digits = np.unique(np.random.randint(low = lowNum, high  = highNum, size = num_words))\n",
    "    while(len(random_digits) < num_words):\n",
    "        random_digits = np.append(random_digits, np.random.randint(low = lowNum, high = highNum, size = num_words-len(random_digits)))\n",
    "        random_digits = np.unique(random_digits)\n",
    "    return random_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia Search Below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start at user given wikipedia page, randomly click a certain number of links from there and scrape 5 sentences from the final page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains text from a wikipedia url\n",
    "def get_wiki_text(url):\n",
    "    # Requesting data from url, finding specialized tags for this particular website\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    #Attaching soup object to page text, obtaining text in paragraphs\n",
    "    soup = bs4.BeautifulSoup(res.text, \"lxml\")\n",
    "    text = \"\"\n",
    "    \n",
    "    # Problematic structure: fails to look for list items which make up substantial amount of wikipedia pages\n",
    "    for paragraph in soup.find_all('p'):\n",
    "        text+= paragraph.text\n",
    "        \n",
    "    # Formatting the string so that it looks normal\n",
    "    text = re.sub(r'\\[.*\\]', '', text)\n",
    "    text= re.sub(r'\\s+', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterates through wikipedia pages\n",
    "def wiki_search():\n",
    "    topic, pages = inp()\n",
    "    textSoup, title = looping_wiki_search(topic, int(pages))\n",
    "    return textSoup, title \n",
    "\n",
    "# Randomly selecting the next topic to be searched for\n",
    "def select_next_topic(text: [str])-> str:\n",
    "    random_number = np.random.randint(1,len(text))\n",
    "    return text[random_number]\n",
    "\n",
    "# Replaces hyphens with an underscore for url purposes, removes all punctuation that could break url\n",
    "def replace_punctuation(text):\n",
    "    specific_case = text.strip()\n",
    "    specific_case = specific_case.replace('-',' ')\n",
    "    specific_case = specific_case.replace(' ', '_')\n",
    "    \n",
    "    # Remove all punctuation\n",
    "    pattern = re.compile(r'\\W')\n",
    "    specific_case = re.sub(pattern, '', specific_case)\n",
    "    \n",
    "    return specific_case\n",
    "    \n",
    "    \n",
    "# Loop through connected topics on wikipedia to find a \"landing page\", then return the text of that landing page. By default, returns the page of 'topic'\n",
    "def looping_wiki_search(topic: str, neighbor_pages = 0):\n",
    "    searchText = \"\"\n",
    "    url = construct_wiki_url(topic)\n",
    "    for i in np.arange(neighbor_pages+1):\n",
    "        print(i)\n",
    "        searchText = get_wiki_text(url).split(\" \")\n",
    "        \n",
    "        \n",
    "        #So long as there are still pages left to proccess\n",
    "        if(i < neighbor_pages):\n",
    "            \n",
    "            # Selecting next topic\n",
    "            topic = select_next_topic(searchText)\n",
    "\n",
    "            # Replacing the punctuation of the next topic\n",
    "            topic = replace_punctuation(topic)\n",
    "\n",
    "            # Moving to next URL\n",
    "            url = construct_wiki_url(topic)\n",
    "        \n",
    "    return searchText, topic\n",
    "        \n",
    "\n",
    "# Temporary dummy input\n",
    "def inp():\n",
    "    print(\"Please enter a topic\")\n",
    "    val = str(input())\n",
    "    print('Please enter the number of neighbor pages: ')\n",
    "    pages = input()\n",
    "    return val, pages\n",
    "\n",
    "# Takes a given user topic and constructs a valid wikipedia url\n",
    "def construct_wiki_url(url_topic : str):\n",
    "    return ('https://en.wikipedia.org/wiki/' + url_topic.lower().strip().replace(\" \",\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a topic\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " zeus\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the number of neighbor pages: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['as,', 'as,', 'a/s', 'or', 'similar', 'may', 'refer', 'to:', ''], 'as')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Scrape for Song Game: Scraping Genius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricsgenius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes artist and song name in terms of string and obtains lyrics\n",
    "def obtain_lyrics(artist: str, song: str) -> [str]:\n",
    "    genius = lyricsgenius.Genius(\"NDltUrlbSis8n9o1FEyGUE_ruIlngdDmXwoQdvrkX0hh3le3LKF8XalcHXOetm3x\")\n",
    "    artist = genius.search_artist(artist, max_songs=3, sort=\"title\")\n",
    "    song = genius.search_song(song, artist.name)\n",
    "    song_lyrics =  song.lyrics\n",
    "    \n",
    "    #String proccessing\n",
    "    song_lyrics = re.sub(r'\\[.*\\]', '', song_lyrics)\n",
    "    song_lyrics  = re.sub(r'\\n+', ' ', song_lyrics)\n",
    "    song_lyrics = song_lyrics.split(\" \")\n",
    "    \n",
    "    #Making sure songs do not exceed maximum threshold\n",
    "    if(len(song_lyrics) > 510):\n",
    "        song_lyrics = song_lyrics[:510]\n",
    "    return song_lyrics\n",
    "\n",
    "# Master Controls obtaining and proccessing lyrics\n",
    "# To do: preserve tags, ie [verse 1], and keep word replacement model from touching it \n",
    "def lyrics(artistName, songName):\n",
    "    lyrics = obtain_lyrics(artistName, songName) \n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for songs by Toto...\n",
      "\n",
      "Song 1: \"21st Century Blues\"\n",
      "Song 2: \"2 Hearts\"\n",
      "Song 3: \"99\"\n",
      "\n",
      "Reached user-specified song limit (3).\n",
      "Done. Found 3 songs.\n",
      "Searching for \"Africa\" by Toto...\n",
      "Done.\n",
      "Searching for songs by Toto...\n",
      "\n",
      "Song 1: \"21st Century Blues\"\n",
      "Song 2: \"2 Hearts\"\n",
      "Song 3: \"99\"\n",
      "\n",
      "Reached user-specified song limit (3).\n",
      "Done. Found 3 songs.\n",
      "Searching for \"Africa\" by Toto...\n",
      "Done.\n",
      "[  2  77  92 165 166 176 182 238 262 266]\n"
     ]
    }
   ],
   "source": [
    "obtain_lyrics('Toto', 'Africa')\n",
    "#text, indices = replace_words_at_random(nlp, obtain_lyrics('Yung Gravy', '1 Thot 2 Thot Red Thot Blue Thot'), 90, -1) # Once this runs, we celebrate\n",
    "text, indices = replace_words_at_random(nlp, obtain_lyrics('Toto', 'Africa'), 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i heard the drums echoing tonight but she hears only whispers of some quiet conversation she's coming in, 12 : 30 flight her moonlit wings reflect the stars that guide me towards salvation i stopped an old man along the way hoping to find some old forgotten words or ancient melodies he turned to me as if to say \" hurry boy, it's waiting there for you \" it's gonna take a lot to drag me away for you there's nothing that a hundred men or more could ever do i bless those rains down in africa gonna take some time to do the things we never had the wild dogs cry out in the night as they grow restless longing for some solitary company i know that i must do what's right as sure as kilimanjaro rises like olympus above the serengeti i seek to cure what's deep inside frightened of this thing that i've become it's gonna take a lot to drag me back to you there's nothing that a hundred men or more can ever do i bless the rain down in africa gonna take some time to do the things we never had hurry boy, she's waiting there for you it's gonna take a lot to drag me away from you there's nothing that a hundred men or more could ever do i bless the rains down in africa i bless the rains down inside africa i bless the rains down in africa i bless the rains down in africa i bless the rains down in africa gonna have some time to see the things we never had\n"
     ]
    }
   ],
   "source": [
    "# Controller for the madlib game, pulls from different data sources to create madlibs based off type\n",
    "def madlib_control(text_type):\n",
    "    pass\n",
    "    \n",
    "# Controller for identify original text, should return original and modified text in random order (option to return multiple?)\n",
    "def identify_original_text(text_type)\n",
    "    pass\n",
    "    \n",
    "def guess_the_page():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
